---
title: "Comparison with lsl Package"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 100)
```
**lsl** is an old package for fitting semi-confirmatory structural equation modeling (SEM) via penalized likelihood (PL).
It implements a expectation-conditional maximization (ECM) algorithm to optimize penalized criteria.
**lsl** can be also seen as a predecessor of **lslx**.
However, the interface and functionality of **lslx** are much improved.

In this document, we try to compare the fitting results made by **lslx** and **lsl** for semi-confirmatory factor analysis.
The data of Holzinger and Swineford (1939) is used.
Since the two packages target the same optimization problem, their results should be quite similar.
Our experiments show that

* **lslx** and **lsl** yield numerically the same results;

* **lslx** is much faster than **lsl**.


### Syntax for Analysis
We first compare the syntax of **lslx** and **lsl** for conducting semi-confirmatory factor analysis.
The code of **lslx** is
```{r comment = "", message = FALSE}
library(lslx)
model <-
'
visual  :=> fix(1)* x1 + x2 + x3
textual :=> fix(1)* x4 + x5 + x6
speed   :=> fix(1)* x7 + x8 + x9
visual  :~> x4 + x5 + x6 + x7 + x8 + x9
textual :~> x1 + x2 + x3 + x7 + x8 + x9
speed   :~> x1 + x2 + x3 + x4 + x5 + x6
'

r6_lslx <- lslx$new(model = model,
                    data = lavaan::HolzingerSwineford1939,
                    verbose = FALSE)

r6_lslx$fit(penalty_method = "mcp",
            lambda_grid = seq(.01, .3, .01),
            delta_grid = 5,
            verbose = FALSE)
```
To understand this code, please see the [vignettes](https://cran.r-project.org/web/packages/lslx/vignettes/factor-analysis.html).

Now we present the code of **lsl** as follows
```{r comment = "", message = FALSE}
library(lsl)
lambda <- matrix(NA, 9, 3)
lambda[c(1, 2, 3), 1] <- lambda[c(4, 5, 6), 2] <- lambda[c(7, 8, 9), 3] <- 1

rc_sem <- lslSEM()
rc_sem$input(raw = lavaan::HolzingerSwineford1939)
rc_sem$specify(pattern = list(lambda = lambda))
rc_sem$learn(penalty = list(type = "mcp", 
                            gamma = seq(.01, .3, .01), 
                            delta = 5), 
             variable = 7:15)
```
Both **lslx** and **lsl** adopts encapsulation object-oriented programming (OOP) paradign.
However, model specification in **lsl** relies on matrix representation, which may not be user-friendly for most SEM users.
Furthermore, the user interface of **lsl** seems to be "annoying": it requires users specify their model and data in multiple stages.
Note that in **lsl** (1) the first freely estimated loading of each latent factor will be automatically fixed to one; 
(2) the regularization parameter for controlling penalty level is `gamma`.

### Fitting Results
To compare the fitting results, we first examine the values of objective function across penalty levels
```{r comment = "", message = FALSE}
objective_value_lsl <- rc_sem$structure$overall["dpl", ,"dt=5"]
objective_value_lslx <- rev(sapply(r6_lslx$get_fitting()$fitted_result$numerical_condition,
                              FUN = function(x) {getElement(x, "objective_value")}))
objective_value_lsl - objective_value_lslx
```
We can see that the two packages result in numerically the same objective function values across all the penalty levels.

Now we compare the values of coefficient estimate.
Here, we only focus on factor loadings.
The estimated loadings made by **lslx** can be easily extracted by `$extract_coefficient_matrice()` method
```{r comment = "", message = FALSE}
r6_lslx$extract_coefficient_matrice(selector = "bic", 
                                    block = "y<-f")
```
**lsl** has no build-in method to extract coefficient matrice.
We can only use `$summarize()` method with `type = "individual"` to see the coefficient estimate.
```{r comment = "", message = FALSE}
rc_sem$summarize(type = "individual")
```
The first 24 rows are loading estimates under AIC selector (2nd column) and BIC selector (3rd column).
Again, we observe that the two packages yield numerically the same loading estimates.

### Computation Time
Finally, we compare the computation time of **lslx** and **lsl**
```{r comment = "", message = FALSE}
fun_lslx <- function() {
  model <-
  '
  visual  :=> fix(1)* x1 + x2 + x3
  textual :=> fix(1)* x4 + x5 + x6
  speed   :=> fix(1)* x7 + x8 + x9
  visual  :~> x4 + x5 + x6 + x7 + x8 + x9
  textual :~> x1 + x2 + x3 + x7 + x8 + x9
  speed   :~> x1 + x2 + x3 + x4 + x5 + x6
  '
  r6_lslx <- lslx$new(model = model,
                      data = lavaan::HolzingerSwineford1939,
                      verbose = FALSE)
  r6_lslx$fit(penalty_method = "mcp",
              lambda_grid = seq(.01, .3, .01),
              delta_grid = 5,
              verbose = FALSE)
}

fun_lsl <- function() {
  lambda <- matrix(NA, 9, 3)
  lambda[c(1, 2, 3), 1] <- lambda[c(4, 5, 6), 2] <- lambda[c(7, 8, 9), 3] <- 1
  rc_sem <- lslSEM()
  rc_sem$input(raw = lavaan::HolzingerSwineford1939)
  rc_sem$specify(pattern = list(lambda = lambda))
  rc_sem$learn(penalty = list(type = "mcp", 
                              gamma = seq(.01, .3, .01), 
                              delta = 5), 
               variable = 7:15)
}

microbenchmark::microbenchmark(
  fun_lslx(),
  fun_lsl(),
  times = 10)
```
We can see that **lslx** is about 80 times faster than **lsl**.
The improvement of **lslx** over **lsl** can be mainly credited to the hidden `C++` code written via **Rcpp** and **RcppEigen**.