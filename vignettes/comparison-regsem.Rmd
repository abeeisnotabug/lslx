---
title: "Comparison with regsem Package"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 100)
```


[**regsem**](https://CRAN.R-project.org/package=regsem) is a package for fitting regularized structural equation modeling.
It can also implements lasso penalty to obtain sparse estimate.

In this document, we try to compare the fitting results made by **lslx** and **regsem**.
The example in **regsem** is adopted, which fits the data of Holzinger and Swineford (1939) by a one-factor model.
Our experiment shows that

* **lslx** and **regsem** yield slightly different results;
* **lslx** is about 20 times faster than **regsem**.

Since the results of **lslx** and **regsem** are inconsistent, [**lsl**](https://CRAN.R-project.org/package=lsl) is also used to fit the data.
We found that **lslx** and **lsl** yield numerically the same results.
By the fact that the objective function values made by **lslx** and **lsl** are smaller then that made by **regsem**, we think that the solution made by current version of **regsem** (0.9.2) can be improved.

Note that our comparison is made based on the following versions of packages.
The comparison result can be different if other versions are used.
```{r comment = "", message = FALSE}
packageVersion("lslx"); packageVersion("regsem"); packageVersion("lsl")
```

### Syntax for Analysis
We first compare the code syntax of **lslx** and **regsem** by using the example in **regsem**.
The code of **lslx** is
```{r comment = "", message = FALSE}
library(lslx)
model_lslx <-
"f1 :=> fix(1) * x1 + pen() * x2 + pen() * x3 + x4 + x5 + x6 + pen() * x7 + pen() * x8 + pen() * x9"

r6_lslx <- lslx$new(model = model_lslx,
                    data = lavaan::HolzingerSwineford1939,
                    verbose = FALSE)

r6_lslx$fit_lasso(lambda_grid = 0.2, verbose = FALSE)
```
In this code, a one-factor model is specified with five loadings to be penalized.
**lslx** fits the model to data with lasso penalty with `lambda = 0.2` - only single penalty level is considered for simplicity.

Now we present the code of **regsem** as follows 
```{r comment = "", message = FALSE}
library(regsem)
model_lavaan <- 
"f =~ 1 * x1 + l1 * x2 + l2 * x3 + l3 * x4 + l4 * x5 + l5 * x6 + l6 * x7 + l7 * x8 + l8 * x9"

fit_lavaan <- lavaan::cfa(model = model_lavaan, 
                          data = lavaan::HolzingerSwineford1939, 
                          meanstructure = TRUE)

fit_regsem <- regsem(fit_lavaan, lambda = 0.1, type = "lasso",
                     pars_pen = c("l1", "l2", "l6", "l7", "l8"),
                     solver = TRUE, quasi = TRUE,
                     solver.maxit = 100, line.search = TRUE)
```
The `regsem()` function uses `lavaan` object as input for further regularized fitting.
Hence, model specification in **regsem** relies on **lavaan** syntax, that is attractive for **lavaan** users.
`lambda = 0.1` in **regsem** is theoretically equivalent to `lambda = 0.2` in **lslx** (see the discussion in section of Fitting Results).
`type = "lasso"` specifies the penalty method and `pars_pen = c("l1", "l2", "l6", "l7", "l8")` specifies which coefficients should be penalized.
Other arguments are used for coordinate descent.

### Fitting Results
The objective function values made by **lslx** and **regsem** are
```{r comment = "", message = FALSE}
# objective function value in lslx (on lslx scale)
r6_lslx$extract_numerical_condition()["objective_value"]
# objective function value in regsem (on regsem scale)
fit_regsem$optim_fit
```
The function values made by **lslx** and **regsem** seem to be on different scale.
We already know the scale of objective function in **lslx**.
To understood the interpretation of the objective function value in **regsem**, we first reconduct analysis with `lambda = 0` and see the objective function value and loss function value
```{r comment = "", message = FALSE}
fit_regsem_0 <- regsem(fit_lavaan, lambda = 0, type = "lasso",
                       pars_pen = c("l1", "l2", "l6", "l7", "l8"))
# objective function value in regsem (on regsem scale)
fit_regsem_0$optim_fit
# loss function value in regsem (on regsem scale)
fit_regsem_0$fit 
```
The objective function value and the loss function value are the same because of `lambda = 0` (i.e., no penalty is implemented).
We suspect that the loss function value in **regsem** is half of ML loss function value, which is justified by the following code
```{r comment = "", message = FALSE}
# chi-square value in lavaan
fitmeasures(fit_lavaan)["chisq"]
# reproduce chi-square value using objective function value in regsem
2 * fit_regsem_0$optim_fit * nrow(lavaan::HolzingerSwineford1939)
# reproduce chi-square value using loss function value in regsem
2 * fit_regsem_0$fit * nrow(lavaan::HolzingerSwineford1939)
```

Now we try to understand the relationship between objective function value, loss function value, and lambda in **regsem**.
The regularizer value in **regsem** can be obtained by
```{r comment = "", message = FALSE}
# regularizer value in regsem (on regsem scale)
regularizer_value <- fit_regsem$optim_fit - fit_regsem$fit
```
The absolute sum of penalized loadings is
```{r comment = "", message = FALSE}
sum_pen_coef <- 
  sum(abs(fit_regsem$coefficients[c("f -> x2", 
                                    "f -> x3", 
                                    "f -> x7", 
                                    "f -> x8", 
                                    "f -> x9")]))
```
Since the regularizer value should equal to the absolute sum of penalized loadings multiplied by lambda, then `regularizer_value / sum_pen_coef` should be `0.1`, the lambda we specified.
```{r comment = "", message = FALSE}
regularizer_value / sum_pen_coef
```
By these examinations, we conclude that the scale of function value in **lslx** is **twice** of the scale of function value in **regsem**, so des the scale of lambda. Therefore, we can compare the objective function values made by **lslx** and **regsem**
```{r comment = "", message = FALSE}
# objective function value in lslx (on the same scale)
r6_lslx$extract_numerical_condition()["objective_value"]
# objective function value in regsem (on the same scale)
2 * fit_regsem$optim_fit
```
By the fact that the objective value made by **regsem** is larger than that made by **lslx**, 
it seems that the solution obtained by **regsem** can be further improved.
The optimality of **lslx** solution can be checked by the sub-gradient of objective function
```{r comment = "", message = FALSE}
r6_lslx$extract_objective_gradient()
```
Note that the largest element in the sub-gradient is for coefficient `x1<-f1|G`, which is a fixed coefficient and hence it is larger than our specified convergence criterion. 

The coefficient estimates yielded by **lslx** and **regsem** are
```{r comment = "", message = FALSE}
r6_lslx$extract_coefficient()
fit_regsem$coefficients
```
We can observe that the estimates are slightly different.

As a double check, we also fit the model to data via **lsl**, a predecessor of **lslx**.
```{r comment = "", message = FALSE}
library(lsl)
lambda <- matrix(NA, 9, 1)
lambda[c(1, 4, 5, 6), 1] <- 1

rc_sem <- lslSEM()
rc_sem$input(raw = lavaan::HolzingerSwineford1939)
rc_sem$specify(pattern = list(lambda = lambda))
rc_sem$learn(penalty = list(type = "l1", 
                            gamma = 0.2, 
                            delta = 5), 
             variable = 7:15)
rc_sem$summarize(type = "overall")["dpl", "bic optimal"]
rc_sem$summarize(type = "individual")[, "bic optimal", drop = FALSE]
```
The objective function value and estimate made by **lsl** is consistent with **lslx**, which further supports that solution of **regsem** can be further improved.

### Computation Time
Finally, we compare the computation time of **lslx** and **regsem**
```{r comment = "", message = FALSE}
fun_lslx <- function() {
model_lslx <-
"f1 :=> fix(1) * x1 + pen() * x2 + pen() * x3 + x4 + x5 + x6 + pen() * x7 + pen() * x8 + pen() * x9"
r6_lslx <- lslx$new(model = model_lslx,
                    data = lavaan::HolzingerSwineford1939,
                    verbose = FALSE)
r6_lslx$fit_lasso(lambda_grid = 0.2, verbose = FALSE)
}

fun_regsem <- function() {
model_lavaan <- 
"f =~ 1 * x1 + l1 * x2 + l2 * x3 + l3 * x4 + l4 * x5 + l5 * x6 + l6 * x7 + l7 * x8 + l8 * x9"
fit_lavaan <- lavaan::cfa(model = model_lavaan, 
                          data = lavaan::HolzingerSwineford1939, 
                          meanstructure = TRUE)
fit_regsem <- regsem(fit_lavaan, lambda = 0.1, type = "lasso",
                     pars_pen = c("l1", "l2", "l6", "l7", "l8"),
                     solver = TRUE, quasi = TRUE,
                     solver.maxit = 100, line.search = TRUE)
}

microbenchmark::microbenchmark(
  fun_lslx(),
  fun_regsem(),
  times = 10)
```
We can see that **lslx** is about 20 times faster than **regsem**.
