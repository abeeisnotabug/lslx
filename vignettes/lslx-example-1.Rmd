---
title: "Example 1: Regression Model with Lasso Penalty"
author: "Po-Hsien Huang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
In this example, we will show how to use `lslx` to conduct regression analysis with lasso penalty.

## Data Generation
The following code is used to generate data for regression analysis. 
```{r comment = "", message = FALSE}
set.seed(9487)
x <- matrix(rnorm(2000), 200, 10)
colnames(x) <- paste0("x", 1:10)
y <- matrix(rnorm(200), 200, 1)
df_reg <- data.frame(y, x)
```
The data set contains 200 observation on 10 covariates (`x1` - `x10`) and a response variable (`y`). 
By the construction of the data, the 10 covariates are not useful to predict the response.
The data is stored in a `data.frame` named `df_reg`.


## Model Sepcification
Model specification in `lslx` is quite similar to that in `lavaan`.
However, different operators and prefix are used to accommodate the presence of penalized parameters.
In the following specification, `y` is predicted by `x1` - `x10`.
```{r comment = "", message = FALSE}
equation1 <-
'
y <= x1 + x2 + x3 + x4
y <~ x5 + x6 + x7 + x8 + x9 + x10
'
```
The operator `<=` means that the regression coefficients from the RHS variables to the LHS variables are freely estimated.
On the other hand, the operator `<~` means that the regression coefficients from the RHS variables to the LHS variables are estimated with penalty.
Details of equation syntax can be found in the section of Equation Syntax via `?lslx`.

## Object Initialization
`lslx` is written as an `R6` class.
Everytime we conduct analysis with `lslx`, an `lslx` object must be initialized.
The following code initializes an `lslx` object named `r6_lslx1`.
```{r comment = "", message = FALSE}
library(lslx)
r6_lslx1 <- lslx$new(equation = equation1,
                     sample_data = df_reg)
```
Here, `lslx` is the object generator for `lslx` object and `new` is the build-in method of `lslx` to generate a new `lslx` object.
The initialization of `lslx` requires users to specify a equation for model specification (argument `equation`) and a data set to be fitted (argument `sample_data`).
The data set must contains all the observed variables specified in the given equation.
In is also possible to initialize an `lslx` object via sample moments (see Example 3).


## Model Fitting
After an `lslx` object is initialized, method `fit` can be used to fit the specified model into the given data.
```{r comment = "", message = FALSE}
r6_lslx1$fit(penalty_method = "lasso",
             lambda_grid = seq(.00, .30, .05))
```
The fitting process requires users to specify the penalty method (argument `penalty_method`) and the considerd penalty levels (argument `lambda_grid`).
In this example, the `lasso` penalty is implemented on the lambda grid `c(.00, .05, .10, .15, .20, .25, .30)`.
All the fitting result will be stored in the `fitting` field of `r6_lslx1`.


## Model Summarizing
Unlike traditional SEM analysis, `lslx` fit the model into data under all the penalty levels considered.
To summarize the fitting result, a selector to determine an optimal penalty level must be specified.
Availble selectors can be found in the section of Penalty Level Selection via `?lslx`.
The following code summarize the fitting result under the penalty level selected by Bayesian information criterion.
```{r comment = "", message = FALSE, fig.width = 24, fig.height = 14}
r6_lslx1$summarize(selector = "bic")
```
So far, the hypothesis testing for the model coefficient are based on the expected Fisher information and doesn't consider the presence of model selection.
In the future, other standard error formula and valid post model selection inference will be available.


## Visualization
`lslx` provides three methods for visualizaing the fitting result.
The method `plot_numerical_condition` shows the numerical condition under all the penalty levels.
The following code plots the values of `n_iter_out` (number of iterations in outer loop), `objective_gradient_abs_max` (maximum of absolute value of gradient of objective function), and `objective_hessian_convexity` (minimum of univariate approximate hessian).
The plot can be used to evaluate the quality of numerical optimization.
```{r comment = "", message = FALSE, fig.width = 8, fig.height = 4, dpi=300, out.width=600, out.height=300}
r6_lslx1$plot_numerical_condition(criterion = c("n_iter_out", 
                                                "objective_gradient_abs_max",
                                                "objective_hessian_convexity"))
```


The method `plot_goodness_of_fit` shows the values of information criteria and fit indices under all the penalty levels.
The following code plots the values of `loss` (ML discrepancy function value evaluated at the current estimate), `aic`, and `bic`.
```{r comment = "", message = FALSE, fig.width = 8, fig.height = 4, dpi=300, out.width=600, out.height=300}
r6_lslx1$plot_goodness_of_fit(selector = c("loss", "aic", "bic"))
```


The method `plot_coefficient` shows the solution path of coefficients in the given block.
The following code plots the solution paths of all coefficients in the block `y<-y`, which contains all the regression coeffcients from observed variables to observed variables.
```{r comment = "", message = FALSE, fig.width = 8, fig.height = 4, dpi=300, out.width=600, out.height=300}
r6_lslx1$plot_coefficient(block = "y<-y")
```
